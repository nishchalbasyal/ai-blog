[
  {
    "objectID": "posts/Univariant Analysis/index.html",
    "href": "posts/Univariant Analysis/index.html",
    "title": "Univariate Analysis : Categorial and Numerical Data",
    "section": "",
    "text": "Univariate analysis focuses on analyzing one variable or column at a time. It provides insights into the characteristics and distribution of individual variables."
  },
  {
    "objectID": "posts/Univariant Analysis/index.html#types-of-graphs-for-different-types-of-data",
    "href": "posts/Univariant Analysis/index.html#types-of-graphs-for-different-types-of-data",
    "title": "Univariate Analysis : Categorial and Numerical Data",
    "section": "Types of Graphs for Different Types of Data",
    "text": "Types of Graphs for Different Types of Data\n\nCategories\n\n\n\nCountplot: A graphical representation of the frequency of categorical data.\n\n\nPie Chart: A circular statistical graphic divided into slices to illustrate numerical proportions.\n\n\n\nNumerical:\n\n\n\nHistogram: A graphical representation of the distribution of numerical data, divided into intervals.\n\n\nDistplot: A plot that displays the probability density function of numerical data, providing insights into the data distribution.\n\n\nBoxplot: A visual summary of the central tendency, dispersion, and skewness of numerical data through five summary statistics.\n\n\nViolin Plot: Combines the features of a box plot and a kernel density plot to visualize the distribution of numerical data and compare distributions across categories.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n\ndf = pd.read_csv('titanic.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\ndf.shape\n\n(891, 12)"
  },
  {
    "objectID": "posts/Univariant Analysis/index.html#categorical",
    "href": "posts/Univariant Analysis/index.html#categorical",
    "title": "Univariate Analysis : Categorial and Numerical Data",
    "section": "Categorical",
    "text": "Categorical\n\nCountplot\nCountplot gives the occurrence (frequency) of categorical data. For example, in Titanic data, it shows the total number of passengers who survived (1) and those who did not survive (0).\nThe sum of all categorical counts will be equal to the total number of rows.\n\nsns.countplot(df,x=\"Survived\")\nplt.show()\n\n\n\n\n\n\n\n\nThis graph denotes that the total number of deaths is over 500, and the number of survivors is over 340, totaling around 850 people (which is the actual size of the dataset).\nSummary: More people died than survived.\n\nsns.countplot(df,x=\"Pclass\")\nplt.show()\n\n\n\n\n\n\n\n\nGraph Description: The majority of people traveled in 3rd class, while the least number of people traveled in 2nd class.\n\nsns.countplot(df,x=\"Sex\")\nplt.show()\n\n\n\n\n\n\n\n\nGraph Description: In the journey, the majority of people are male.\n\nsns.countplot(df,x=\"Embarked\")\nplt.show()\n\n\n\n\n\n\n\n\nGraph Description: Most people departed from Southampton (S), followed by Cherbourg (C) and Queenstown (Q).\n\n\nPie Chart\nA pie chart is ideal for displaying categorical data in percentages, dividing a circle into sectors to represent proportions visually. It effectively showcases relative proportions within a dataset and is easy to understand.\n\ndf['Survived'].value_counts().plot(kind='pie',autopct=\"%.2f\")\n\n\n\n\n\n\n\n\n\ndf['Embarked'].value_counts().plot(kind='pie',autopct=\"%.2f\")"
  },
  {
    "objectID": "posts/Univariant Analysis/index.html#numerical-data",
    "href": "posts/Univariant Analysis/index.html#numerical-data",
    "title": "Univariate Analysis : Categorial and Numerical Data",
    "section": "Numerical Data",
    "text": "Numerical Data\n\nHistogram\nA histogram is used to analyze the distribution of continuous data. It provides insights into how data is spread across different ranges or bins, aiding in understanding its central tendency and variability. While histograms are typically used for continuous data, they can also be adapted to represent categorical data by grouping values into intervals.\n\nplt.hist(df['Age'],bins=15)\n\n(array([ 44.,  24.,  32., 104., 115., 127.,  80.,  59.,  49.,  30.,  22.,\n         17.,   4.,   6.,   1.]),\n array([ 0.42      ,  5.72533333, 11.03066667, 16.336     , 21.64133333,\n        26.94666667, 32.252     , 37.55733333, 42.86266667, 48.168     ,\n        53.47333333, 58.77866667, 64.084     , 69.38933333, 74.69466667,\n        80.        ]),\n &lt;BarContainer object of 15 artists&gt;)\n\n\n\n\n\n\n\n\n\nFrom this graph, we can see that individuals aged between 18 and 32 are the most frequent travelers on the ship. The age distribution ranges from a minimum of 0 to a maximum of 80, with data divided into 15 bins.\n\n\ndistplot/Histplot\nThis plot displays the probability density function rather than the total count, as seen in a traditional histogram. It provides a visual representation of the distribution of data, focusing on the probability of occurrence for different values rather than their frequency.\n\nsns.histplot(df['Age'],kde=True, stat=\"density\")\n\n\n\n\n\n\n\n\nThis graph illustrates the probability of specific ages. For instance, it answers questions like: What is the probability of being age 40? The answer is 0.015, indicating a 1.5% probability of being age 40.\n\n\nBoxplot\nA boxplot provides the five-number summary of a dataset: 1) Minimum: Represents the lowest value within the dataset, calculated as Q1 - 1.5 * IQR. 2) Interquartile Range (IQR): Measures the spread of the middle 50% of the data. 3) Q1 (25th Percentile): Represents the value below which 25% of the data falls. 4) Q3 (75th Percentile): Represents the value below which 75% of the data falls. 5) Maximum: Calculated as Q3 + 1.5 * IQR, it helps identify outliers or noise in the data.\nBoxplots are valuable for visualizing the distribution of data and detecting outliers efficiently.\n\nsns.boxplot(df['Age'])\n\n\n\n\n\n\n\n\nThis graph aids in identifying outliers and determining the fifth number. For instance, ages above 65 are considered outliers. Additionally:\n\n75% of people are below the age of 38.\n50% of people are below the age of 28.\n25% of people are below the age of 20.\n\n\n# This can be verify by using this methods\ndf['Age'].describe()\n\ncount    714.000000\nmean      29.699118\nstd       14.526497\nmin        0.420000\n25%       20.125000\n50%       28.000000\n75%       38.000000\nmax       80.000000\nName: Age, dtype: float64"
  },
  {
    "objectID": "posts/Univariant Analysis/index.html#extra-for-numerical",
    "href": "posts/Univariant Analysis/index.html#extra-for-numerical",
    "title": "Univariate Analysis : Categorial and Numerical Data",
    "section": "Extra for Numerical",
    "text": "Extra for Numerical\n\nViolin Plots\nViolin plots are a method of plotting numeric data and can be considered a combination of the box plot and kernel density plot.\nOne advantage of the violin plot over the box plot is that, aside from displaying the five-number summary, it shows the entire distribution of the data. This allows for a more comprehensive understanding of the data distribution and facilitates comparison of the distribution of a given variable across different categories.\n\nsns.violinplot(df['Age'])\n\n\n\n\n\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=df,x='Age',hue='Sex')\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=df,x='Age',hue='Survived')"
  },
  {
    "objectID": "posts/confidence_interval/index.html",
    "href": "posts/confidence_interval/index.html",
    "title": "Confidence Intervals Using Z-Procedure and T-Procedure.",
    "section": "",
    "text": "Confidence Intervals, in simple words, represent a range of values where the particular population parameters (e.g., Mean) to fall.\n\n\nConfidence Levels indicate how confident we are that the true values lie within the interval, denoted by a percentage, e.g., 95%.\n\nConfidence Interval = Point Estimate Â± Margin of Error\n\nThere are two ways to obtain Confidence Intervals:\n\n\n\nZ-Procedure (Sigma Known)\n\n\nT-Procedure"
  },
  {
    "objectID": "posts/confidence_interval/index.html#z--procedure",
    "href": "posts/confidence_interval/index.html#z--procedure",
    "title": "Confidence Intervals Using Z-Procedure and T-Procedure.",
    "section": "Z- Procedure",
    "text": "Z- Procedure\n\nThis method is used for populations whose variance is known.\n\n Assumptions:\n\n\nMust Have Population Standard Deviation\n\n\nRandom Sampling\n\n\nPopulation Distribution must be a Normal Distribution or follow Central Limit Theorem\n\n\nFormula: \\[ C.I = \\bar{X} \\pm Z_{\\alpha / 2} \\times \\frac{\\sigma}{\\sqrt{n}} \\]\n\n\n\nWhere:\n\n\n\n\n\n\\(\\bar{X}\\)\nPoint of Estimation or Mean of samples\n\n\n\\(Z_{\\alpha / 2}\\)\nZ-value for Confidence Level\n\n\n\\(\\sigma\\)\nStandard Deviation of population\n\n\n\\(n\\)\nSample Size\n\n\n\nWe mostly estimated using a 95% confidence level for our estimations, allowing us to formulate the following formula: \\[ C.I = \\bar{X} \\pm 1.96 \\times \\frac{\\sigma}{\\sqrt{n}} \\]\nWe Use a Salary Dataset for Demonstration. Letâs Take Salary Column To Predict our Population Parameters.\n\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('Salary.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nAge\nGender\nEducation Level\nJob Title\nYears of Experience\nSalary\n\n\n\n\n0\n32.0\nMale\nBachelor's\nSoftware Engineer\n5.0\n90000.0\n\n\n1\n28.0\nFemale\nMaster's\nData Analyst\n3.0\n65000.0\n\n\n2\n45.0\nMale\nPhD\nSenior Manager\n15.0\n150000.0\n\n\n3\n36.0\nFemale\nBachelor's\nSales Associate\n7.0\n60000.0\n\n\n4\n52.0\nMale\nMaster's\nDirector\n20.0\n200000.0\n\n\n\n\n\n\n\n\nLetâs check the Distribution of Salary\n\nprint(df['Salary'].describe())\n\ncount       373.000000\nmean     100577.345845\nstd       48240.013482\nmin         350.000000\n25%       55000.000000\n50%       95000.000000\n75%      140000.000000\nmax      250000.000000\nName: Salary, dtype: float64\n\n\n\ndf['Salary'].plot(kind='kde')\n\n\n\n\n\n\n\n\nLetâs be more Clear by Using QQPlot\n\nstats.probplot(df['Salary'],dist='norm',plot=plt)\nplt.plot()\n\n\n\n\n\n\n\n\n\ndf['Salary'].skew()\n\n0.400578053273342\n\n\nNow Letâs check through central limit theorem.\n\nsample_salary = []\nfor i in range(10):\n    sample_salary.append(df['Salary'].dropna().sample(50).values.tolist())\n\n\nsample_salary = np.array(sample_salary)\n\n\nsample_salary.shape\n\n(10, 50)\n\n\n\nsample_means = np.mean(sample_salary, axis=1)\n\n\nimport seaborn as sns\n\n\nsns.kdeplot(sample_means)\n\n\n\n\n\n\n\n\nNow itâs approximately follow normal distribution.\nNow all Conditions Satisfy.  Letâs Calculate Confidence Intervals using Z- Procedure.\n\n# Note in Z_Procedure We have to know the Standard Deviation of Population\nstandard_deviation = np.std(df['Salary'])\n\n\nstandard_deviation\n\n48175.30518517429\n\n\n\nlower_limit = np.mean(sample_means) - 1.96 * (standard_deviation/np.sqrt(50))\nupper_limit = np.mean(sample_means) + 1.96 * (standard_deviation/np.sqrt(50))\n\n\nprint(\"The Range is  :\",lower_limit,'-',upper_limit)\n\nThe Range is  : 90027.88668699007 - 116734.91331300992\n\n\nAt a 95% confidence level, the range for the confidence interval is [84058.58668699008 - 110765.61331300993].\nThis statement indicates that we are 95% confident that the true population parameter falls within the range of 84058.58668699008 to 110765.61331300993.\n\n# To verify\npopulation_mean = df['Salary'].mean()\n\nprint(\"The actual population mean:\" ,population_mean,\"lies within a confident interval range\") if population_mean &gt;= lower_limit and population_mean &lt;= upper_limit else print(\"False\")\n\nThe actual population mean: 100577.34584450402 lies within a confident interval range"
  },
  {
    "objectID": "posts/confidence_interval/index.html#t--procedure",
    "href": "posts/confidence_interval/index.html#t--procedure",
    "title": "Confidence Intervals Using Z-Procedure and T-Procedure.",
    "section": "T- Procedure",
    "text": "T- Procedure\nThis method is used for populations whose variance is unknown, and itâs a commonly employed procedure for estimating population parameters.\n\nAssumptions:\n\nRandom Sampling.\nPopulation Distribution must be a Normal Distribution or follow Central Limit Theorem.\nIndependent Observations.\nWe donât know the standard deviation of Salaries\n\nFormula: \\[ C.I = \\bar{X} \\pm t_{\\alpha / 2} \\times \\frac{S}{\\sqrt{n}} \\]\n\n\n\nWhere:\n\n\n\n\n\n\\(\\bar{X}\\)\nPoint of estimation or Mean of samples\n\n\n\\(t_{\\alpha / 2}\\)\nt-value for confidence Level\n\n\n\\(S\\)\nSample mean of Standard Deviation\n\n\n\\(n\\)\nSample size\n\n\n\nNote: \\(t_{\\alpha / 2}\\) t-value for confidence Level which depends on degree of freedom (n-1)\nFor example, if we aim for a 95% confidence level and have a sample size of 40 for estimation, we can formulate the following formula (From T-table): \\[ C.I = \\bar{X} \\pm 2.021 \\times \\frac{\\sigma}{\\sqrt{n}} \\]\n\n# samples were created by using 40 sample size of each\nsample_salary = []\nfor i in range(10):\n    sample_salary.append(df['Salary'].dropna().sample(40).values.tolist())\n    \nsample_salary = np.array(sample_salary)\n\nsample_means = np.mean(sample_salary,axis=1) # mean of each 10 samples\n\nsample_salary_std = np.std(sample_salary,axis=1) # standard deviation of each 10 samples\n\nsample_mean_of_standard_deviation = np.mean(sample_salary) # mean of all sample standard deviations\n\n\n# By Using formula\nlower_limit = np.mean(sample_means) - 2.021 * (sample_mean_of_standard_deviation/np.sqrt(40))\nupper_limit = np.mean(sample_means) + 2.021 * (sample_mean_of_standard_deviation/np.sqrt(40))\n\n\nprint(\"The Range is  :\",lower_limit,'-',upper_limit)\n\nThe Range is  : 68743.83817322378 - 133309.66182677622\n\n\n\n# To verify\npopulation_mean = df['Salary'].mean()\n\nprint(\"The actual population mean:\" ,population_mean,\"lies within a calculated confident interval range: \",lower_limit,'-',upper_limit) if population_mean &gt;= lower_limit and population_mean &lt;= upper_limit else print(\"False\")\n\nThe actual population mean: 100577.34584450402 lies within a calculated confident interval range:  68743.83817322378 - 133309.66182677622\n\n\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(x=1, y=np.mean(sample_means), yerr=2.021 * (sample_mean_of_standard_deviation/np.sqrt(40)), fmt='o', color='blue', label='Mean Salary with Confidence Interval')\nplt.scatter(x=np.ones_like(sample_means), y=sample_means, color='red', label='Sampled Means')\nplt.errorbar(x=1, y=lower_limit, fmt='o', color='green', label='Lower Limit')\nplt.errorbar(x=1, y=upper_limit, fmt='o', color='purple', label='Upper Limit')\nplt.xticks([1], ['Means'])\nplt.ylabel('Salary')\nplt.title('Sampled Means with Confidence Interval')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Confidence Intervals Using Z-Procedure and T-Procedure.\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\n\n\n\n\n\nFeature Engineering : FunctionTransformer and PowerTransformer\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nUnivariate Analysis : Categorial and Numerical Data\n\n\n\n\n\n\n\n\nMay 4, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I am Nishchal Basyal. An AI Aspirant passionate about sharing knowledge on Machine Learning, Deep Learning, Neural Networks, Computer Vision, and NLP."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Nishchal Basyalâs AI Journey",
    "section": "",
    "text": "Exploring the realms of Artificial Intelligence"
  },
  {
    "objectID": "posts/feature_engineering_transformation/index.html",
    "href": "posts/feature_engineering_transformation/index.html",
    "title": "Feature Engineering : FunctionTransformer and PowerTransformer",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\n\n\ndf = pd.read_csv('titanic.csv',usecols=['Age','Fare','Survived'])\n\n\ndf['Age'].fillna(df['Age'].mean(),inplace=True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nSurvived\nAge\nFare\n\n\n\n\n0\n0\n22.0\n7.2500\n\n\n1\n1\n38.0\n71.2833\n\n\n2\n1\n26.0\n7.9250\n\n\n3\n1\n35.0\n53.1000\n\n\n4\n0\n35.0\n8.0500\n\n\n\n\n\n\n\n\n\nX = df.iloc[:,1:3]\ny = df.iloc[:,0]\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n\n\nplt.figure(figsize=(14,4))\nplt.subplot(121)\nsns.histplot(X_train['Age'],kde='True')\nplt.title('Age')\n\nplt.subplot(122)\nstats.probplot(X_train['Age'],dist='norm',plot=plt)\nplt.title('Age QQ Plot')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(14,4))\nplt.subplot(121)\nsns.histplot(X_train['Fare'],kde='True')\nplt.title('Age')\n\nplt.subplot(122)\nstats.probplot(X_train['Fare'],dist='norm',plot=plt)\nplt.title('Age QQ Plot')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nclf = LogisticRegression()\nclf2 = DecisionTreeClassifier()\n\n\nclf.fit(X_train,y_train)\nclf2.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\ny_pred1 = clf2.predict(X_test)\n\nprint(\"Accuracy LR\", accuracy_score(y_test,y_pred))\nprint(\"Accuracy DT\", accuracy_score(y_test,y_pred1))\n\nAccuracy LR 0.6480446927374302\nAccuracy DT 0.6759776536312849\n\n\n\ntrf = FunctionTransformer(func=np.log1p)\n\n\nX_train_transformed = trf.fit_transform(X_train)\nX_test_tranformed = trf.transform(X_test)\n\n\nclf = LogisticRegression()\nclf2 = DecisionTreeClassifier()\n\nclf.fit(X_train_transformed,y_train)\nclf2.fit(X_train_transformed,y_train)\n\ny_pred = clf.predict(X_test_tranformed)\ny_pred1 = clf2.predict(X_test_tranformed)\n\nprint(\"Accuracy LR\", accuracy_score(y_test,y_pred))\nprint(\"Accuracy DT\", accuracy_score(y_test,y_pred1))\n\nAccuracy LR 0.6815642458100558\nAccuracy DT 0.6759776536312849\n\n\n\nX_transformed = trf.fit_transform(X)\n\nclf = LogisticRegression()\nclf2 = DecisionTreeClassifier()\n\nprint(\"LR\",np.mean(cross_val_score(clf,X_transformed,y,scoring='accuracy',cv=10)))\nprint(\"DT\",np.mean(cross_val_score(clf2,X_transformed,y,scoring='accuracy',cv=10)))\n\n\nLR 0.678027465667915\nDT 0.6633208489388265\n\n\n\nplt.figure(figsize=(14,4))\n\nplt.subplot(121)\nstats.probplot(X_train['Age'],dist=\"norm\",plot=plt)\nplt.title('Age Before Log')\n\nplt.subplot(122)\nstats.probplot(X_transformed['Age'],dist='norm',plot=plt)\nplt.title('Age After log')\n\nplt.show()\n\n\n\n\n\n\n\n\n\ntrf2 = ColumnTransformer([('log',FunctionTransformer(np.log1p),['Fare'])],remainder='passthrough')\n\nX_train_transformed2 = trf2.fit_transform(X_train)\nX_test_transformed2 = trf2.transform(X_test)\n\n\nclf = LogisticRegression()\nclf2 = DecisionTreeClassifier()\n\nclf.fit(X_train_transformed2,y_train)\nclf2.fit(X_train_transformed2,y_train)\n\ny_pred = clf.predict(X_test_transformed2)\ny_pred1 = clf2.predict(X_test_transformed2)\n\nprint(\"Accuracy LR\", accuracy_score(y_test,y_pred))\nprint(\"Accuracy DT\", accuracy_score(y_test,y_pred1))\n\nAccuracy LR 0.6703910614525139\nAccuracy DT 0.6703910614525139\n\n\n\ndef apply_transform(transform):\n    X = df.iloc[:,1:3]\n    y = df.iloc[:,0]\n\n    trf = ColumnTransformer([('log',FunctionTransformer(transform),['Fare'])],remainder='passthrough')\n\n    X_transformed = trf.fit_transform(X)\n\n    clf = LogisticRegression()\n\n    print('Accuracy',np.mean(cross_val_score(clf,X_transformed,y,scoring='accuracy',cv=10)))\n\n    plt.figure(figsize=(14,4))\n\n    plt.subplot(121)\n    stats.probplot(df['Fare'],dist='norm',plot=plt)\n    plt.title('Fare Before Transform')\n\n    plt.subplot(122)\n    stats.probplot(X_transformed[:,0],dist='norm',plot=plt)\n    plt.title('Fare After Transform')\n\n    plt.show()\n\n\napply_transform(lambda x:x) #nothing\n\nAccuracy 0.6589013732833957\n\n\n\n\n\n\n\n\n\n\napply_transform(lambda x:x**2) #left skew (Square transform)\n\nAccuracy 0.6431335830212235\n\n\n\n\n\n\n\n\n\n\napply_transform(lambda x:x**1/2) # (Square Root)\n\nAccuracy 0.6589013732833957\n\n\n\n\n\n\n\n\n\n\napply_transform(lambda x:1/(x+0.00001)) # (Reciprocal Transform)\n\nAccuracy 0.61729088639201\n\n\n\n\n\n\n\n\n\n\napply_transform(np.sin) # (Custom Transform)\n\nAccuracy 0.6195131086142323\n\n\n\n\n\n\n\n\n\n\nPower Transform\n\ndf = pd.read_csv(\"concrete_data.csv\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nCement\nBlast Furnace Slag\nFly Ash\nWater\nSuperplasticizer\nCoarse Aggregate\nFine Aggregate\nAge\nStrength\n\n\n\n\n0\n540.0\n0.0\n0.0\n162.0\n2.5\n1040.0\n676.0\n28\n79.99\n\n\n1\n540.0\n0.0\n0.0\n162.0\n2.5\n1055.0\n676.0\n28\n61.89\n\n\n2\n332.5\n142.5\n0.0\n228.0\n0.0\n932.0\n594.0\n270\n40.27\n\n\n3\n332.5\n142.5\n0.0\n228.0\n0.0\n932.0\n594.0\n365\n41.05\n\n\n4\n198.6\n132.4\n0.0\n192.0\n0.0\n978.4\n825.5\n360\n44.30\n\n\n\n\n\n\n\n\n\ndf.shape\n\n(1030, 9)\n\n\n\ndf.isnull().sum()\n\nCement                0\nBlast Furnace Slag    0\nFly Ash               0\nWater                 0\nSuperplasticizer      0\nCoarse Aggregate      0\nFine Aggregate        0\nAge                   0\nStrength              0\ndtype: int64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nCement\nBlast Furnace Slag\nFly Ash\nWater\nSuperplasticizer\nCoarse Aggregate\nFine Aggregate\nAge\nStrength\n\n\n\n\ncount\n1030.000000\n1030.000000\n1030.000000\n1030.000000\n1030.000000\n1030.000000\n1030.000000\n1030.000000\n1030.000000\n\n\nmean\n281.167864\n73.895825\n54.188350\n181.567282\n6.204660\n972.918932\n773.580485\n45.662136\n35.817961\n\n\nstd\n104.506364\n86.279342\n63.997004\n21.354219\n5.973841\n77.753954\n80.175980\n63.169912\n16.705742\n\n\nmin\n102.000000\n0.000000\n0.000000\n121.800000\n0.000000\n801.000000\n594.000000\n1.000000\n2.330000\n\n\n25%\n192.375000\n0.000000\n0.000000\n164.900000\n0.000000\n932.000000\n730.950000\n7.000000\n23.710000\n\n\n50%\n272.900000\n22.000000\n0.000000\n185.000000\n6.400000\n968.000000\n779.500000\n28.000000\n34.445000\n\n\n75%\n350.000000\n142.950000\n118.300000\n192.000000\n10.200000\n1029.400000\n824.000000\n56.000000\n46.135000\n\n\nmax\n540.000000\n359.400000\n200.100000\n247.000000\n32.200000\n1145.000000\n992.600000\n365.000000\n82.600000\n\n\n\n\n\n\n\n\n\nX = df.drop(columns=['Strength'])\ny = df.iloc[:,-1]\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nr2_score(y_test,y_pred)\n\n0.6275531792314848\n\n\n\nlr = LinearRegression()\nnp.mean(cross_val_score(lr,X,y,scoring='r2'))\n\n0.4609940491662864\n\n\n\nfor col in X_train.columns:\n    plt.figure(figsize=(14,4))\n    plt.subplot(121)\n    sns.histplot(X_train[col],kde=True,element='step')\n\n    plt.subplot(122)\n    stats.probplot(X_train[col],dist=\"norm\",plot=plt)\n    plt.title(col)\n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import PowerTransformer\n\n\n# Applying Box-Cox Transform\n\npt = PowerTransformer(method='box-cox')\n\nX_train_transformed = pt.fit_transform(X_train+0.000001)\nX_test_transformed = pt.transform(X_test+0.000001)\n\npd.DataFrame({'cols':X_train.columns,'box_cox_lambdas':pt.lambdas_})\n\n\n\n\n\n\n\n\n\ncols\nbox_cox_lambdas\n\n\n\n\n0\nCement\n0.177025\n\n\n1\nBlast Furnace Slag\n0.025093\n\n\n2\nFly Ash\n-0.038970\n\n\n3\nWater\n0.772682\n\n\n4\nSuperplasticizer\n0.098811\n\n\n5\nCoarse Aggregate\n1.129813\n\n\n6\nFine Aggregate\n1.782019\n\n\n7\nAge\n0.066631\n\n\n\n\n\n\n\n\n\nlr = LinearRegression()\nlr.fit(X_train_transformed,y_train)\n\ny_pred2 = lr.predict(X_test_transformed)\n\nr2_score(y_test,y_pred2)\n\n0.8047825007854219\n\n\n\nX_train_transformed = pd.DataFrame(X_train_transformed,columns=X_train.columns)\nfor col in X_train_transformed.columns:\n    plt.figure(figsize=(14,4))\n    plt.subplot(121)\n    sns.histplot(X_train[col],kde=True,element='step')\n    plt.title(col)\n\n    plt.subplot(122)\n    sns.histplot(X_train_transformed[col],kde=True,element='step')\n    plt.title(col)\n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Applying Yeo-Johnson Transform\n\npt1 = PowerTransformer()\n\nX_train_transformed2 = pt1.fit_transform(X_train+0.000001)\nX_test_transformed2 = pt1.transform(X_test+0.000001)\n\nlr = LinearRegression()\nlr.fit(X_train_transformed2,y_train)\n\ny_pred3 = lr.predict(X_test_transformed2)\n\nprint(r2_score(y_test,y_pred3))\n\n\npd.DataFrame({'cols':X_train.columns,'box_cox_lambdas':pt1.lambdas_})\n\n0.8161906545593434\n\n\n\n\n\n\n\n\n\n\ncols\nbox_cox_lambdas\n\n\n\n\n0\nCement\n0.174348\n\n\n1\nBlast Furnace Slag\n0.015715\n\n\n2\nFly Ash\n-0.161447\n\n\n3\nWater\n0.771307\n\n\n4\nSuperplasticizer\n0.253935\n\n\n5\nCoarse Aggregate\n1.130050\n\n\n6\nFine Aggregate\n1.783100\n\n\n7\nAge\n0.019885\n\n\n\n\n\n\n\n\n\nX_train_transformed2 = pd.DataFrame(X_train_transformed2,columns=X_train.columns)\nfor col in X_train_transformed2.columns:\n    plt.figure(figsize=(14,4))\n    plt.subplot(121)\n    sns.histplot(X_train[col],kde=True,element='step')\n    plt.title(col)\n\n    plt.subplot(122)\n    sns.histplot(X_train_transformed2[col],kde=True,element='step')\n    plt.title(col)\n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npd.DataFrame({'cols':X_train.columns,'box_cox_lambdas':pt.lambdas_,'Yeo_Johnson_lambdas':pt1.lambdas_})\n\n\n\n\n\n\n\n\n\ncols\nbox_cox_lambdas\nYeo_Johnson_lambdas\n\n\n\n\n0\nCement\n0.177025\n0.174348\n\n\n1\nBlast Furnace Slag\n0.025093\n0.015715\n\n\n2\nFly Ash\n-0.038970\n-0.161447\n\n\n3\nWater\n0.772682\n0.771307\n\n\n4\nSuperplasticizer\n0.098811\n0.253935\n\n\n5\nCoarse Aggregate\n1.129813\n1.130050\n\n\n6\nFine Aggregate\n1.782019\n1.783100\n\n\n7\nAge\n0.066631\n0.019885"
  }
]